{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = r\"https://www.sec.gov/\"\n",
    "fullIndex_url = r\"https://www.sec.gov/Archives/edgar/full-index/\"\n",
    "\n",
    "\n",
    "normal_url = r\"https://www.sec.gov/Archives/edgar/data/1265107/0001265107-19-000004.txt\"\n",
    "json_url = normal_url.replace('-','').replace('.txt','/index.json')\n",
    "\n",
    "documents_url = r\"https://www.sec.gov/Archives/edgar/data/1265107/000126510719000004/index.json\"\n",
    "\n",
    "content = requests.get(documents_url).json()\n",
    "\n",
    "for file in content['directory']['item']:\n",
    "    \n",
    "    # Grab the filing summary and create a new url leading to the file so we can download it.\n",
    "    if file['name'] == 'FilingSummary.xml':\n",
    "\n",
    "        xml_summary = base_url + content['directory']['name'] + \"/\" + file['name']\n",
    "        \n",
    "        print('-' * 100)\n",
    "        print('File Name: ' + file['name'])\n",
    "        print('File Path: ' + xml_summary)\n",
    "        \n",
    "        \n",
    "# define a new base url that represents the filing folder. This will come in handy when we need to download the reports.\n",
    "base_url = xml_summary.replace('FilingSummary.xml', '')\n",
    "\n",
    "# request and parse the content\n",
    "content = requests.get(xml_summary).content\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "# find the 'myreports' tag because this contains all the individual reports submitted.\n",
    "reports = soup.find('myreports')\n",
    "\n",
    "# I want a list to store all the individual components of the report, so create the master list.\n",
    "master_reports = []\n",
    "\n",
    "# loop through each report in the 'myreports' tag but avoid the last one as this will cause an error.\n",
    "for report in reports.find_all('report')[:-1]:\n",
    "\n",
    "    # let's create a dictionary to store all the different parts we need.\n",
    "    report_dict = {}\n",
    "    report_dict['name_short'] = report.shortname.text\n",
    "    report_dict['name_long'] = report.longname.text\n",
    "    report_dict['position'] = report.position.text\n",
    "    report_dict['category'] = report.menucategory.text\n",
    "    report_dict['url'] = base_url + report.htmlfilename.text\n",
    "    report_dict['xml'] = base_url + report['instance']\n",
    "\n",
    "    # append the dictionary to the master list.\n",
    "    master_reports.append(report_dict)\n",
    "    \n",
    "# create the list to hold the statement urls\n",
    "statements_url = []\n",
    "xml_url = []\n",
    "\n",
    "# define the statements we want to look for.\n",
    "item1 = r\"Consolidated Balance Sheets\"\n",
    "item2 = r\"Consolidated Statements of Operations and Comprehensive Income (Loss)\"\n",
    "item3 = r\"Consolidated Statements of Cash Flows\"\n",
    "item4 = r\"Consolidated Statements of Stockholder's (Deficit) Equity\"\n",
    "\n",
    "# store them in a list.\n",
    "report_list = [item1, item2, item3, item4]\n",
    "\n",
    "for report_dict in master_reports:\n",
    "    \n",
    "    # if the short name can be found in the report list.\n",
    "    if report_dict['name_short'] in report_list:\n",
    "        \n",
    "#         # print some info and store it in the statements url.\n",
    "#         print('-'*100)\n",
    "#         print(report_dict['name_short'])\n",
    "#         print(report_dict['url'])\n",
    "#         print(report_dict['xml'])\n",
    "        \n",
    "        statements_url.append(report_dict['url'])\n",
    "        xml_url.append(report_dict['xml'])\n",
    "        \n",
    "for statement in [statements_url[0]]: # Should do all statements, but focus on balance sheet for now\n",
    "\n",
    "    # request the statement file content\n",
    "    content = requests.get(statement).content\n",
    "    report_soup = BeautifulSoup(content, 'html')\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for row in report_soup.table.find_all('tr'):\n",
    "        \n",
    "        if row.th:\n",
    "            cols = row.find_all('th')\n",
    "            data['name'] = cols[0].text.strip()\n",
    "            data['years'] = [element.text.strip() for element in cols[1:]]\n",
    "            continue\n",
    "            \n",
    "\n",
    "        if row.find_all('strong'): # Means a section head\n",
    "            cols = row.find_all('td')\n",
    "            key = cols[0].text.strip()\n",
    "            data[key] = {}\n",
    "\n",
    "        if row.find_all('strong').__len__() == 0:\n",
    "            cols = [element.text.strip() for element in row.find_all('td')]\n",
    "            subcategory = cols[0]\n",
    "            data[key][subcategory] = cols[1:]\n",
    "            \n",
    "with open('./data/bs/balance.txt', 'w') as file:\n",
    "    file.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df = pd.DataFrame()\n",
    "rawdata = {}\n",
    "\n",
    "fullIndex_url = r\"https://www.sec.gov/Archives/edgar/full-index/\"\n",
    "\n",
    "fullIndex = json.loads(requests.get(fullIndex_url + r\"index.json\").content)\n",
    "\n",
    "for year in fullIndex['directory']['item']:\n",
    "    \n",
    "    year_url = fullIndex_url + year['href']\n",
    "    yearIndex = json.loads(requests.get(year_url + r\"index.json\").content)\n",
    "    print(\"Parsing data for \" + year['name'] )\n",
    "    \n",
    "    for quarter in yearIndex['directory']['item']:\n",
    "        \n",
    "#         print(year['name']+quarter['name'])\n",
    "        \n",
    "        quarter_url = year_url + quarter['href']\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Request by URL, decode, and read into pandas DF\n",
    "            raw_df = pd.read_csv(io.StringIO(requests.get(quarter_url + r\"master.idx\").content.decode('utf-8')),\n",
    "                    delimiter = '|',\n",
    "                    skiprows = [0,1,2,3,4,5,6,7,8,10], # Standard format for .idx file\n",
    "                    index_col=0\n",
    "                   )\n",
    "            \n",
    "            # Insert raw data into dictionary for later use\n",
    "            rawdata[ str(year['name'] + quarter['name']) ] = raw_df\n",
    "            \n",
    "            # Pull only 10-Q & 10-K forms\n",
    "            df = raw_df[raw_df['Form Type'].str.match(r\"10-[QK]$\")][\"Form Type\"].str.extract(r\"([QK])\")\n",
    "            df = df.groupby(\"CIK\").aggregate({0: ', '.join})\n",
    "            df.columns = [ year['name']+ quarter['name']]\n",
    "\n",
    "            reference_df = df.join(reference_df)\n",
    "\n",
    "        except:\n",
    "            print(year['name']+quarter['name'] + \" did not contain data.\")\n",
    "            df = pd.DataFrame(index=['CIK'],columns=[ year['name']+ quarter['name']])\n",
    "        \n",
    "        try:\n",
    "            reference_df = df.join(reference_df, how='left')\n",
    "        except:\n",
    "            print('Could not join ' + year['name'] + quarter['name'])\n",
    "#         print(reference_df)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "# year = json.loads(requests.get(fullIndex_url + fullIndex['directory']['item'][0]['href'] + r\"index.json\").content)\n",
    "# quarter = json.loads(requests.get(fullIndex + year['directory']['item'][0]['href'] +  + r\"index.json\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc3 in position 13013584: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d07ec2f3ceee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m pd.read_csv(io.StringIO(requests.get(r\"https://www.sec.gov/Archives/edgar/full-index/2011/QTR4/\" + r\"master.idx\").content.decode('utf-8')),\n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'|'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mskiprows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                    )\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc3 in position 13013584: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "pd.read_csv(io.StringIO(requests.get(r\"https://www.sec.gov/Archives/edgar/full-index/2011/QTR4/\" + r\"master.idx\").content.decode('utf-8')),\n",
    "                    delimiter = '|',\n",
    "                    skiprows = [0,1,2,3,4,5,6,7,8,10],\n",
    "                    index_col=0\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1993QTR3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CIK</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60512</th>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66740</th>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1993QTR3\n",
       "CIK           \n",
       "60512        Q\n",
       "66740        Q"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
